import os
import pandas as pd
from evaluate import load
from sklearn.metrics import accuracy_score, f1_score

# === åŠ è½½è¯„ä¼°å™¨ ===
rouge = load("rouge")
bertscore = load("bertscore")

# === æ¨¡å‹è·¯å¾„é…ç½® ===
base_dir = "results/medical"
model_names = ["åŸå§‹åŸºåº§æ¨¡å‹", "Fæ€§æ ¼æ¨¡å‹", "Tæ€§æ ¼æ¨¡å‹"]

summary_results = {}
sentiment_results = {}

# === å¸¸ç”¨å‡½æ•° ===
def normalize(x):
    return str(x).strip().strip(".").lower()

valid_labels = {"normal", "depression"}

def resolve_prediction(raw_pred, gold_label):
    pred = normalize(raw_pred)
    if pred in valid_labels:
        return pred
    elif "depression" in pred:
        return "depression"
    elif "normal" in pred:
        return "normal"
    else:
        # æ— æ³•è¯†åˆ« â†’ å¼ºåˆ¶åˆ¤é”™
        return "normal" if gold_label == "depression" else "depression"

# === æ‘˜è¦ä»»åŠ¡ï¼ˆMeQSumï¼‰===
for model in model_names:
    path = os.path.join(base_dir, model, "meqsum_results.csv")
    if not os.path.exists(path):
        continue

    df = pd.read_csv(path)
    preds = df["prediction"].astype(str).tolist()
    refs = df["Summary"].astype(str).tolist()

    r_scores = rouge.compute(predictions=preds, references=refs)
    b_scores = bertscore.compute(predictions=preds, references=refs, lang="en")
    avg_bert = {
        "BERTScore_P": sum(b_scores["precision"]) / len(b_scores["precision"]),
        "BERTScore_R": sum(b_scores["recall"]) / len(b_scores["recall"]),
        "BERTScore_F1": sum(b_scores["f1"]) / len(b_scores["f1"]),
    }

    summary_results[model] = {**r_scores, **avg_bert}

# === æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼ˆMental Sentimentï¼‰===
for model in model_names:
    path = os.path.join(base_dir, model, "mental_sentiment_results.csv")
    if not os.path.exists(path):
        continue

    df = pd.read_csv(path)
    raw_labels = df["label"].map(normalize).tolist()
    raw_preds = df["prediction"].astype(str).tolist()

    resolved_preds = []
    for pred, label in zip(raw_preds, raw_labels):
        resolved_preds.append(resolve_prediction(pred, label))

    acc = accuracy_score(raw_labels, resolved_preds)
    f1 = f1_score(raw_labels, resolved_preds, pos_label="depression", average="binary")

    sentiment_results[model] = {
        "accuracy": acc,
        "f1": f1,
        "count": len(raw_labels)
    }

# === å†™å…¥ metrics.txt ===
lines = []

lines.append("===== Mental Sentiment åˆ†ç±»ä»»åŠ¡ =====")
for model, scores in sentiment_results.items():
    lines.append(f"\nğŸ“Œ Model: {model}")
    for k, v in scores.items():
        lines.append(f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}")
    lines.append("-" * 40)

lines.append("\n===== MeQSum æ‘˜è¦ä»»åŠ¡ =====")
for model, scores in summary_results.items():
    lines.append(f"\nğŸ“Œ Model: {model}")
    for k, v in scores.items():
        lines.append(f"{k}: {v:.4f}")
    lines.append("-" * 40)

os.makedirs(base_dir, exist_ok=True)
with open(os.path.join(base_dir, "metrics.txt"), "w") as f:
    f.write("\n".join(lines))
