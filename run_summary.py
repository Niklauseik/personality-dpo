import os
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# === è®¾å¤‡é…ç½® ===
device = "cuda" if torch.cuda.is_available() else "cpu"

# === æ¨¡å‹è·¯å¾„é…ç½® ===
model_configs = {
    "åŸå§‹åŸºåº§æ¨¡å‹": "./llama-3B-Instruct",
    "Fæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_f_3B",
    "Tæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_t_3B"
}

# === æ•°æ®è·¯å¾„ä¸ä»»åŠ¡å®šä¹‰ ===
datasets = {
    "edtsum": {
        "path": "datasets/finbench/edtsum.csv",
        "input_col": "query",
        "output_dir": "results/finbench",
        "prompt_template": lambda text: (
            f"Summarize the following content in no more than 3 sentences.\n\n{text}")
    },
    "movie": {
        "path": "datasets/movie/wiki_movie_summ_3k.csv",
        "input_col": "Plot",
        "output_dir": "results/movie_summary",
        "prompt_template": lambda text: (
            f"Summarize the movie plot below in no more than 4 sentences.\n\n{text}")
    }
}

# === æ¨ç†å‡½æ•° ===
def local_generate(prompt, tokenizer, model):
    messages = [{"role": "user", "content": prompt}]
    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    gen_kwargs = dict(
        max_new_tokens=128,
        do_sample=True,
        temperature=0.3,
        top_p=0.9,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    with torch.no_grad():
        out = model.generate(**inputs, **gen_kwargs)

    generated = out[0][inputs["input_ids"].shape[-1]:]
    return tokenizer.decode(generated, skip_special_tokens=True).strip()

# === ä¸»æµç¨‹ï¼šæ¯ä¸ªæ¨¡å‹è·‘æ‰€æœ‰æ‘˜è¦ä»»åŠ¡ ===
for model_name, model_path in model_configs.items():
    print(f"\nğŸ§ª æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼š{model_name}")

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    ).eval()

    for task_name, task_info in datasets.items():
        print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†æ•°æ®é›†ï¼š{task_name}")
        df = pd.read_csv(task_info["path"])
        input_col = task_info["input_col"]
        prompt_func = task_info["prompt_template"]

        predictions = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"{model_name} - {task_name}"):
            raw_text = str(row[input_col]).strip()
            prompt = prompt_func(raw_text)
            try:
                pred = local_generate(prompt, tokenizer, model)
            except Exception as e:
                pred = f"[Error] {e}"
            predictions.append(pred)

        df_result = df.copy()
        df_result["prediction"] = predictions

        save_dir = os.path.join(task_info["output_dir"], model_name)
        os.makedirs(save_dir, exist_ok=True)
        df_result.to_csv(os.path.join(save_dir, f"{task_name}_results.csv"), index=False, encoding="utf-8")
        print(f"âœ… ä¿å­˜å®Œæˆï¼š{task_name} â†’ {save_dir}")
