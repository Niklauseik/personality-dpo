import os
import pandas as pd
import re
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# === æ ¹ç›®å½•ï¼Œç¡®ä¿æ˜¯åœ¨ personality/ ä¸‹è¿è¡Œ ===
base_path = "./results/benchmark"

# === æ¨¡å‹æ–‡ä»¶å¤¹åç§° ===
model_folders = ["Fæ€§æ ¼æ¨¡å‹", "Tæ€§æ ¼æ¨¡å‹", "åŸå§‹åŸºåº§æ¨¡å‹"]

# === æ•°æ®é›†æ–‡ä»¶åæ˜ å°„ï¼ˆæ–°å¢ GSM8Kï¼‰ ===
files = {
    "ARC (easy)": "arc_easy_test800_results.csv",
    "BoolQ": "boolq_train800_results.csv",
    "GSM8K": "gsm8k_test800_results.csv"
}

# === æå–å‡½æ•° ===
def extract_upper_letter(text):
    match = re.search(r'\b([A-D])\b', str(text).upper())
    return match.group(1) if match else None

def extract_bool(text):
    if isinstance(text, str):
        text_lower = text.lower()
        if 'true' in text_lower:
            return True
        elif 'false' in text_lower:
            return False
    elif isinstance(text, bool):
        return text
    return None

def compute_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    return {
        "accuracy": round(accuracy, 4),
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1": round(f1, 4)
    }

# === æ”¶é›†æ‰€æœ‰ç»“æœ ===
all_results = []

for model_name in model_folders:
    model_path = os.path.join(base_path, model_name)

    for dataset_name, filename in files.items():
        file_path = os.path.join(model_path, filename)
        if not os.path.exists(file_path):
            continue

        df = pd.read_csv(file_path)

        if dataset_name == "ARC (easy)":
            df["label_clean"] = df["label"].apply(extract_upper_letter)
            df["prediction_clean"] = df["prediction"].apply(extract_upper_letter)
        elif dataset_name == "BoolQ":
            df["label_clean"] = df["label"].apply(extract_bool)
            df["prediction_clean"] = df["prediction"].apply(extract_bool)
        elif dataset_name == "GSM8K":
            df["label_clean"] = df["label"].astype(str).str.strip()
            df["prediction_clean"] = df["prediction"].astype(str).str.strip()

        df_valid = df.dropna(subset=["label_clean", "prediction_clean"])

        if dataset_name == "GSM8K":
            accuracy = accuracy_score(df_valid["label_clean"], df_valid["prediction_clean"])
            metrics = {
                "accuracy": round(accuracy, 4),
                "precision": None,
                "recall": None,
                "f1": None
            }
        else:
            metrics = compute_metrics(df_valid["label_clean"], df_valid["prediction_clean"])

        all_results.append({
            "Model": model_name,
            "Dataset": dataset_name,
            **metrics
        })

# === è¾“å‡ºä¸º DataFrame ç»“æœè¡¨ ===
df_metrics = pd.DataFrame(all_results)
print(df_metrics)

# === ä¿å­˜ç»“æœåˆ° txt æ–‡ä»¶ ===
output_path = os.path.join(base_path, "benchmark_metrics_summary.txt")

with open(output_path, "w", encoding="utf-8") as f:
    for _, row in df_metrics.iterrows():
        f.write(
            f"\nğŸ“Œ Model: {row['Model']}\n"
            f"ğŸ“Š Dataset: {row['Dataset']}\n"
            f"âœ… Accuracy: {row['accuracy']}\n"
            f"âœ… Precision: {row['precision']}\n"
            f"âœ… Recall: {row['recall']}\n"
            f"âœ… F1 Score: {row['f1']}\n"
            f"{'-'*40}\n"
        )

print(f"\nğŸ“ å·²å°†ç»“æœä¿å­˜åˆ°ï¼š{output_path}")
