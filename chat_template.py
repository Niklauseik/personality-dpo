from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… ä¸‰ä¸ªæ¨¡å‹è·¯å¾„ï¼ˆbase / T / Fï¼‰
model_paths = {
    "Fæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_f_3B",
    "Tæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_t_3B",
    "åŸå§‹åŸºåº§æ¨¡å‹": "./llama-3B-Instruct"
}

# âœ… é—®é¢˜
messages = [
    {"role": "user", "content": "å‡å¦‚ä½ è€ƒè¯•å¤±åˆ©äº†ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ"}
]

# âœ… æ¨ç†å‚æ•°
gen_kwargs = dict(
    max_new_tokens=1024,
    do_sample=True,
    temperature=0.2,
    top_p=0.8,
    repetition_penalty=1.1
)

# âœ… éå†æ¨¡å‹æµ‹è¯•
for name, model_path in model_paths.items():
    print(f"\n=== ğŸ” æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼š{name} ===")

    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    ).eval()

    # æ„é€  prompt
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        return_attention_mask=True
    ).to(device)

    # ç”Ÿæˆ
    with torch.no_grad():
        out = model.generate(
            **inputs,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
            **gen_kwargs
        )

    # è§£ç æ–°ç”Ÿæˆéƒ¨åˆ†
    generated = out[0][ inputs["input_ids"].shape[-1]: ]
    answer = tokenizer.decode(generated, skip_special_tokens=True).strip()

    print(f"ğŸ—¨ï¸ å›ç­”ï¼š{answer}")
