import os
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"

# === æ¨¡å‹è·¯å¾„é…ç½® ===
model_configs = {
    "åŸå§‹åŸºåº§æ¨¡å‹": "./llama-3B-Instruct",
    "Fæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_f_3B",
    "Tæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_t_3B"
}

# === åŠ è½½æ•°æ®é›†ï¼ˆä¿æŒåŸå­—æ®µï¼‰===
movie_data = pd.read_csv("datasets/movie/wiki_movie_summ_3k.csv")   # åˆ—ï¼šPlot, PlotSummary
imdb_data = pd.read_csv("datasets/movie/imdb_test.csv")             # åˆ—ï¼štext, label

# === æ¨ç†å‡½æ•° ===
def local_generate(prompt, tokenizer, model):
    messages = [{"role": "user", "content": prompt}]
    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    gen_kwargs = dict(
        max_new_tokens=256,
        do_sample=True,
        temperature=0.2,
        top_p=0.8,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
    generated = outputs[0][inputs["input_ids"].shape[-1]:]
    return tokenizer.decode(generated, skip_special_tokens=True).strip()

# === ä¸»æµç¨‹ï¼šæ‰€æœ‰æ¨¡å‹ Ã— 2ä¸ªä»»åŠ¡ ===
for model_name, model_path in model_configs.items():
    print(f"\nğŸ§ª æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼š{model_name}")

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    ).eval()

    # === Movie æ‘˜è¦ç”Ÿæˆ ===
    movie_predictions = []
    for _, row in tqdm(movie_data.iterrows(), total=len(movie_data), desc=f"{model_name} - Movie"):
        prompt = (
            "You are a helpful assistant that writes short summaries for movie plots.\n\n"
            f"Plot:\n{row['Plot']}\n\nSummary:"
        )
        try:
            pred = local_generate(prompt, tokenizer, model)
        except Exception as e:
            pred = f"[Error] {e}"
        movie_predictions.append(pred)

    movie_result = movie_data.copy()
    movie_result["prediction"] = movie_predictions
    movie_out_dir = os.path.join("results", "movie_summary", model_name)
    os.makedirs(movie_out_dir, exist_ok=True)
    movie_result.to_csv(os.path.join(movie_out_dir, "movie_summary_results.csv"), index=False, encoding="utf-8")
    print(f"âœ… Movie æ‘˜è¦ç»“æœå·²ä¿å­˜ï¼š{movie_out_dir}")

    # === IMDb æƒ…æ„Ÿåˆ†ç±» ===
    imdb_predictions = []
    for _, row in tqdm(imdb_data.iterrows(), total=len(imdb_data), desc=f"{model_name} - IMDb"):
        prompt = (
            "You are a movie review sentiment classifier. Respond with only one word: positive or negative.\n\n"
            f"Review:\n{row['text']}\n\nSentiment:"
        )
        try:
            pred = local_generate(prompt, tokenizer, model)
        except Exception as e:
            pred = f"[Error] {e}"
        imdb_predictions.append(pred)

    imdb_result = imdb_data.copy()
    imdb_result["prediction"] = imdb_predictions
    imdb_out_dir = os.path.join("results", "imdb_sentiment", model_name)
    os.makedirs(imdb_out_dir, exist_ok=True)
    imdb_result.to_csv(os.path.join(imdb_out_dir, "imdb_sentiment_results.csv"), index=False, encoding="utf-8")
    print(f"âœ… IMDb ç»“æœå·²ä¿å­˜ï¼š{imdb_out_dir}")
