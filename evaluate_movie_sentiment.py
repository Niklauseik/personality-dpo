import os
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# === é…ç½®è·¯å¾„ä¸æ¨¡å‹ ===
base_dir = "results/imdb_sentiment"
models = ["åŸå§‹åŸºåº§æ¨¡å‹", "Fæ€§æ ¼æ¨¡å‹", "Tæ€§æ ¼æ¨¡å‹"]
result_file = "imdb_sentiment_results.csv"

# === æ˜ å°„æ¨¡å‹è¾“å‡ºä¸º 0 æˆ– 1 ===
def map_sentiment_label(text):
    if not isinstance(text, str):
        return None
    text = text.lower()
    if "positive" in text:
        return 1
    if "negative" in text:
        return 0
    return None  # å…¶ä»–ç±»åˆ«æˆ–æ— æ³•è¯†åˆ«çš„è¿”å› None

# === ä¸»æµç¨‹ ===
output_path = os.path.join(base_dir, "imdb_sentiment_metrics.txt")
with open(output_path, "w", encoding="utf-8") as f_out:
    for model in models:
        path = os.path.join(base_dir, model, result_file)
        if not os.path.exists(path):
            print(f"âŒ ç¼ºå¤±ç»“æœæ–‡ä»¶ï¼š{path}")
            continue

        df = pd.read_csv(path)
        gold_labels = df["label"].tolist()
        predictions_raw = df["prediction"].tolist()

        predictions = [map_sentiment_label(p) for p in predictions_raw]
        valid = [(y, p) for y, p in zip(gold_labels, predictions) if p is not None]

        if not valid:
            f_out.write(f"\nğŸ“Œ Model: {model}\n")
            f_out.write("accuracy: nan\nprecision: 0.0\nrecall: 0.0\nf1: 0.0\ncount: 0\n")
            f_out.write("-" * 40 + "\n")
            continue

        y_true, y_pred = zip(*valid)

        metrics = {
            "accuracy": round(accuracy_score(y_true, y_pred), 4),
            "precision": round(precision_score(y_true, y_pred), 4),
            "recall": round(recall_score(y_true, y_pred), 4),
            "f1": round(f1_score(y_true, y_pred), 4),
            "count": len(y_true)
        }

        f_out.write(f"\nğŸ“Œ Model: {model}\n")
        for k, v in metrics.items():
            f_out.write(f"{k}: {v}\n")
        f_out.write("-" * 40 + "\n")

print(f"\nâœ… æ‰€æœ‰ IMDb æƒ…æ„Ÿåˆ†ç±»è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜è‡³ï¼š{output_path}")
