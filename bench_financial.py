import os
import re
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"

# === æ¨¡å‹è·¯å¾„ï¼ˆbase / F / Tï¼‰===
model_configs = {
    "åŸå§‹åŸºåº§æ¨¡å‹": "./llama-3B-Instruct",
    "Fæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_f_3B",
    "Tæ€§æ ¼æ¨¡å‹": "./dpo_outputs/model_t_3B"
}

# === FinBench æ•°æ®é›†ï¼ˆè¯»å–å…¨éƒ¨ï¼‰===
finbench_datasets = {
    "german_400": pd.read_csv("datasets/finbench/german_700.csv"),
    "cfa_1000": pd.read_csv("datasets/finbench/cfa_1000.csv"),
    "fiqasa": pd.read_csv("datasets/finbench/fiqasa.csv"),
    "bigdata_1400": pd.read_csv("datasets/finbench/bigdata_1400.csv"),
    "headlines_2000": pd.read_csv("datasets/finbench/headlines_2000.csv")
}

# === æ¨ç†å‡½æ•° ===
def local_generate(prompt, tokenizer, model):
    messages = [{"role": "user", "content": prompt}]
    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

    gen_kwargs = dict(
        max_new_tokens=128,
        do_sample=True,
        temperature=0.2,
        top_p=0.8,
        repetition_penalty=1.5,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    with torch.no_grad():
        out = model.generate(**inputs, **gen_kwargs)

    generated = out[0][inputs["input_ids"].shape[-1]:]
    return tokenizer.decode(generated, skip_special_tokens=True).strip()

# === ä¸»æµç¨‹ï¼šæ‰€æœ‰æ¨¡å‹ Ã— æ‰€æœ‰æ•°æ®é›† ===
for model_name, model_path in model_configs.items():
    print(f"\nğŸ§ª æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼š{model_name}")

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    ).eval()

    for dataset_name, df in finbench_datasets.items():
        print(f"\nğŸ“‚ å¤„ç†æ•°æ®é›†ï¼š{dataset_name}")
        predictions = []

        for idx, row in df.iterrows():
            raw_text = str(row["text"]).strip()
            cleaned = re.sub(r'\bAnswer\s*[:ï¼š]?\s*$', '', raw_text, flags=re.IGNORECASE).strip()

            # === æ„é€  prompt ===
            if dataset_name == "german_400":
                prompt = f"{cleaned}\n\nOnly respond with one word: good or bad. For example: good"
            elif dataset_name == "cfa_1000":
                prompt = f"{cleaned}\n\nOnly respond with one word. For example: C"
            elif dataset_name == "fiqasa":
                prompt = (
                    "You are a financial sentiment classifier. "
                    "Respond with only one word: either 'positive', 'neutral', or 'negative'.\n\n"
                    f"{cleaned}"
                )
            elif dataset_name == "bigdata_1400":
                prompt = (
                    f"{cleaned}\n\nKindly confirm with only one word: Rise or Fall. For example: Rise"
                )
            elif dataset_name == "headlines_2000":
                prompt = (
                    f"{cleaned}\n\nOnly respond with one word: Yes or No. For example: No"
                )
            else:
                prompt = cleaned

            try:
                response = local_generate(prompt, tokenizer, model)
            except Exception as e:
                response = f"[Error] {e}"

            predictions.append(response)

        # ä¿å­˜ç»“æœ
        df_result = df.copy()
        df_result["prediction"] = predictions
        save_dir = os.path.join("results", "finbench", model_name)
        os.makedirs(save_dir, exist_ok=True)
        df_result.to_csv(os.path.join(save_dir, f"{dataset_name}_results.csv"), index=False, encoding="utf-8")
        print(f"âœ… ä¿å­˜å®Œæˆï¼š{dataset_name} â†’ {save_dir}")
